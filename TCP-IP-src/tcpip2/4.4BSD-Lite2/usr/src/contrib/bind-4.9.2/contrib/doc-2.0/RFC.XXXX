/*
** Distributed with 'doc' Version 2.0 from University of Southern
** California Information Sciences Institute (USC-ISI). 8/22/90
*/

Network Working Group                                 Steve Hotz
Request for Comments: XXXX                            Paul Mockapetris
                                                      XX/YY/ZZ

         PRELIMINARY DRAFT & NOTES: NOT YET RELEASED/REVIEWED

                       Automated Domain Testing


1. Status:

This RFC suggests ideas for the automated testing of DNS domains for
correct configuration.  The purpose of this RFC is to elicit further
discussion regarding system requirements and issues of design and
implementation.

Distribution of this memo is unlimited.


2. Introduction:

The Domain Name System, currently used by a majority of the Internet's
component networks, is a large and complex, autonomously administrated,
distributed database which provides a network wide name service.  It's
size, and the autonomously administered nature of the system, make it
an ideal breeding ground for problems caused by misconfigured domains.

An automated tool for checking that domains are consistent with the
procedures as specified in the official RFCs would prove useful in
a couple ways.

It would be most useful as a means for domain administrators to verify
(to a certain degree) that their domains are configured correctly.
This would allow new domains to become operational fairly quickly,
while ensuring that the domain is not causing far-reaching problems
during the period when the administrator is still gaining hands-on
experience.

The development of such a tool would also codify the interpretation
of what is considered correct or incorrect, and serve as a yardstick
against which strangely behaving domains may be measured.

Additionally, by examining misconfiguration trends of the Internet
as a whole, one might identify areas in which the DNS, or various
implementations, may need to evolve in future versions (i.e. perhaps
administrators often configure their domains in a certain "incorrect"
manner to gain some particular behavior or characteristic not
achievable with standard "correct" configuration).



3.1.  On what scope should a question be asked?

There are a few variants on the question that one would like such a
tool to answer.  The simplest query would be whether or not a specific
domain is correctly configured.  In light of the primary intended use
this tool, this question is probably sufficient.

However, one might like to ask about the configuration of the entire
domain tree.  A generalization of this would be to ask about a
specified subtree (which could be a leaf domain).  However, pursuing
this question requires walking the domain tree by searching through
all zone information for zone delegations.  This would cause a
considerable amount of network traffic, and you may be thwarted by
servers unwilling to give up entire zone information.

Although the latter may more convenient when checking a group of
domains, and possibly more efficient with respect to network and
nameserver utilization (by not repeating queries to common parent
domain servers), we will assume the former model throughout this RFC.


3.2.  What should be tested?  General Discussion.

The most interesting task in designing such a tool is determining what
constitutes a misconfigured domain, which problems we should look for,
and finally, which misconfigurations are possible/reasonable to detect
with an automated procedure.

Because the point at which a zone delegation is made is prone to
errors, and the "delegation" and "acceptance" of authority spans two
distinct domains, a test of a single domain should probably look at
information from multiple domains.

One could, in addition to looking at the particular domain in
question, attempt to look at children (delegated) domains. However,
this again raises the issue of determining which domains (if any) are
below a given domain.  On the other hand, one can test the parent
(delegating) domain to verify that it's configuration is consistent
with respect to the domain under scrutiny.

We will assume the latter approach for now.  Again, whether one
approach or the other is more useful, is a topic for discussion.
I believe that a tool based on either model could be used to discover
the same set of problems in any arbitrary subtree, by choosing the
appropriate set of domains to examine.


3.4. What claims should be made?

Once we have decided on a set of conditions we wish to test, we need
to specify what one should say about anomalies that are detected.
Should there be an attempt to classify them in some manner?  If so, a
classification might be similar to the following:


ERRORS:   Something is definitely wrong, and should be fixed.
          Example: Authoritative nameservers for a domain disagree
          on information about the domain.

WARNINGS: Used for less serious problems or strong indicators of
          problems not decidably testable.  Example: Parent domain
          servers have differing SOA records.  A sign of a change
          to domain information which hasn't propagated yet.
          May explain some of the other errors.

NOTE:     One may be able to determine some potential problems by
          looking at information returned as a matter of course in
          investigating some separate problem.  If such potential
          problems don't occur as often as the indicator, you may
          choose to note it rather than incur additional overhead
          investigating a likely dead-end.  Example: Glue records
          were not returned with NS query to non-authoritative
          server.  May not be an error since returning them in
          "ADDITIONAL" section is optional.


3.5. Procedure Variants

When subsequent queries are based on the results of a query, there
are often several possible ways to proceed.

Example:  Assume that we are querying the parent domain servers to
determine a set of authoritative servers for the domain.  The response
to these queries will be used to form a list of the appropriate
servers to which another query will be posed.  In some instances,
among the set of servers for the parent domain, some may claim to also
be authoritative for the domain being tested.  Should you treat these
replies differently than replies from parent servers that are not
authoritative for the domain?  A few alternatives might be: (1) Ignore
the non-authoritative replies, only if authoritative responses were
received.  (2) Classify nameservers for the domain based on whether
parent nameserver holding an NS record for them were authoritative or
not, and further, look at results from nameservers of different
classes differently.  (3) Most simply, you might consider any
nameserver for the domain specified in an NS record from any parent
domain server to be equivalent.

Similar issues will exist for other parts of the procedure.


3.6.  What should be tested?  Specific examples.

The following is a partial set of anomalies to consider for automated
domain testing.


3.6.1.  Queries to parent domain servers about domain.

a. Server claims to be non-authoritative for parent domain.
   An initial query will probably be made to discover the nameservers
   for the parent domain.  If one of these nameservers is not
   authoritative, it probably should not have been listed among the
   NS referrals.

b. Server has no SOA record for parent domain.
   As above (a), this server should not have been listed as a
   nameserver.

c. Server returns multiple SOAs for parent domain.

d. SOA records from different servers have different serial numbers.
   All servers should, except during short transition interval between
   updates, have consistent data.

e. SOA records from different servers are different (same serial no).
   Changes to important information is being changed, but version is
   not being updated; possibly preventing distribution of changes.



3.6.2.  Queries to parent domain servers about domain.

f. NS records from single server have different TTLs.
   Server has information, possibly inconsistent, from incorrect
   sources or out-of-date versions.

g. Glue records present (or not present).
   Parent server should have addresses of authoritative servers;
   particularly important if non-authoritative.

h. List of NS records for domain differ among servers.
   Parent servers should have consistent delegation information
   for the domain.

i. Server claims to also be authoritative for domain.
   This is certainly not infrequent, however we might want to test
   whether other authoritative domain servers concur.   Also, often
   the NS records from domain servers and from parent domain servers
   disagree, even if both authoritative.  May want to note whether
   inconsistent NS lists are of this special case.


3.6.3.  Queries to domain servers about domain.

j. Server has no SOA record for domain.
   Parent servers have incorrect data and are referring queries to
   about the domain to an unsuspecting server.

k. Server claims to be non-authoritative.
   As above (j), parent server believes server to be authoritative and
   is referring others to this server.

l. Server returns multiple SOAs for domain.
   See above (c).

m. SOA records from different servers have different serial numbers.
   See above (d).

n. SOA records from different servers are different (but same serial).
   See above (e).

o. NS records from single server have different TTLs.
   See above (f).

p. List of NS records for domain differ among servers.
   All authoritative servers for a domain should agree on information
   about domain, particularly important delegation records.

q. NS list from parent servers does not match list from authoritative
   servers.
   Correctly operating domains will have identical NS records at all
   domains servers and parents domain servers.

r. Server claims to be authoritative, however no NS record
   from authoritative servers.
   Strangely enough, this happens often when a server claims to be
   authoritative, answers questions about the domain, but does not
   have an NS record for itself.

s. Reverse mapping (in-addr.arpa.) for server address not found.
   A frequent error in newly established domains.


4. Implementation:

An initial implementation attempt is available via anonymous
ftp from venera.isi.edu, file: pub/doc.tar.Z

Doc requires the latest release of 'dig' (version 2.0) to perform
nameserver queries;  dig is also available: pub/dig.2.0.tar.Z.
This software is intended to run on Berkeley UNIX (and variants)
machines.


4.1. Current procedure (Doc-V.1.0):

  A: Abort test -- can't continue.
  E: Incorrect behavior is considered an error.
  W: Incorrect behavior in this respect causes warning to be issued.
  N: Note occurrence/information.
  o: Side effects ... additional "computation".

Start test:

(1) Query default nameserver for NS records of parent domain.

(2) Query servers for parent domain for SOA record for parent domain.

    W: Check each response to see that it was authoritative.
    W: Check each response to see that SOA records were returned.
    W: Check that only one SOA was returned.

    W: Check that SOA serial numbers are same from all servers.
       Only servers not issued warning above are tested here.

    A: No server returned an SOA record.

    o  Generate list of parent servers that are authoritative and
       returned correct SOA information.  This is the list of servers
       that are asked next query.
  
  At this point, might want to add check to compare entire SOA.
  Such a check for parent SOAs probably not relative enough for
  test of child domain.  Similar reasoning why above are warnings
  and not errors.


(3) Query all authoritative servers of parent zone for
    NS records of domain being tested.

    N: Note the number of NS records and A records corresponding
       to nameserver (glue) were returned in response.

    E: Check that TTLs of NS records are the same.

    o: Determine if response is happens to be authoritative for
       testee domain.  Information is kept separate depending if
       came from authoritative server.
       This gets sort of messy, and may not be necessary (partially
       leftover from earlier versions).  However, many domains have
       different information at non-authoritative and authoritative.
       This allows one to be a bit more specific in issuing errors
       about what set of servers had inconsistent data.

    E: Check that NS records from different servers agree.
        (Test is done separately for the AUTH and non-AUTH
        server's lists.  If both are consistent, then check
        if the two lists also agree.)

    o: Generate list of servers for testee domain.
        Include any with corresponding NS record from any
        parent server (regardless to authority of server).
        Other lists are also maintained:
          - servers known by authoritative parent servers
          - servers known by non-authoritative parent servers
          - servers only known by non-authoritative parent servers
        Might also want to look at those only known by authoritative.

    W: Look at each parent server that also claimed authority for
       domain -- check that an NS record is held for it (by any
       of the servers).

(4) Query set of nameservers for testee domain for SOA records for
    domain.  Currently, set is generated above and includes any
    nameserver for which an NS record was returned in the above
    series (3) of queries.  Different criteria for set inclusion
    may also be interesting.

    E: Check each response to see that it was authoritative.
    E: Check each response to see that SOA records were returned.
    W: Check that only one SOA was returned.

    E: Check that SOA serial numbers are same from all servers.
       Only servers not issued warning above are tested here.

    E: Check that entire SOA record matches among servers.
       (Checked only if serial numbers agree).

    o: Generate list of nameservers that are authoritative
       and have at least one SOA record.


(5) Query set of testee domain nameservers for NS records of domain.
    Currently, this set includes all nameservers which in previous
    series of queries, returned authoritative response containing
    exactly one SOA.

    E: Check that TTLs of NS records are the same.

    E: Check that NS records from servers are the same.

    E: Check that NS records from testee serves agree with
       NS records from parent domain servers (make comparison with
       any list consistent among some set of the parent servers --
       i.e. remember that AUTH/non-AUTH mess !!)
       This is only checked if child servers agree among themselves.
     check for agreement between parent and child servers.

    E: Check that all servers that claim to be authoritative
       have NS record at held by one of the AUTH servers.

Generate a list of addresses of nameservers for domain the domain.
Choose addresses of servers that are in the domain in question
(i.e. don't care about some other domain's server which is acting
as a secondary).  Currently, we only look at one address on per any
single network (i.e. only of 128.9.0.32 and 128.9.0.33 would be
followed up).


(6) Query for in-addr.arpa. PTR records for list of addresses
    on networks of the domain.

    E:  Check that response is returned to reverse mapping query.


4.2. Example test runs:

Note: The domains have been changed to protect the miscreants.
      Output has been changed to fit the RFC.


4.2.1.  Test for mystery domain #1:

Doc-1.0: Starting test of mystery.dom.   parent is edu.
Doc-1.0: Test date - Fri Apr 27 14:57:05 PDT 1990
soa @a.isi.edu. for edu. has serial: 900423
DIGERR (TIME_OUT): dig @aos.brl.mil. for SOA of parent failed
soa @c.nyser.net. for edu. has serial: 900423
soa @gunter-adam.af.mil. for edu. has serial: 900423
soa @ns.nasa.gov. for edu. has serial: 900423
DIGERR (TIME_OUT): dig @ns.nic.ddn.mil. for SOA of parent failed
soa @terp.umd.edu. for edu. has serial: 900426
WARNING: Found 2 unique SOA serial #'s for edu.
Found 3 NS and 3 glue records for mystery.dom. @a.isi.edu. 
Found 3 NS and 3 glue records for mystery.dom. @c.nyser.net. 
Found 3 NS and 3 glue records for mystery.dom. @gunter-adam.af.mil.
Found 3 NS and 3 glue records for mystery.dom. @ns.nasa.gov.
Found 3 NS and 3 glue records for mystery.dom. @terp.umd.edu.
DNServers for edu.
   === 0 were also authoritatve for mystery.dom.
   === 5 were non-authoritative for mystery.dom.
Servers for edu. (not also authoritative for mystery.dom.)
   === agree on NS records for mystery.dom.
NS list summary for mystery.dom. from parent (edu.) servers
  == mystery.dom. cs.mystery.dom. pendragon.cs.purdue.edu.
soa @mystery.dom. for mystery.dom. serial: 900425
soa @cs.mystery.dom. for mystery.dom. serial: 900425
soa @pendragon.cs.purdue.edu. for mystery.dom. serial: 900425
SOA serial #'s agree for mystery.dom.
Authoritative domain (mystery.dom.) servers agree on NS for mystery.dom.
ERROR: NS list from mystery.dom. servers do not match parent
  === (edu.) NS list
NS list summary for mystery.dom. from authoritative servers
  == telcom.mystery.dom.
ERROR: mystery.dom. claims to be AUTH no NS record from AUTH servers
ERROR: cs.mystery.dom. claims to be AUTH no NS record
===   from AUTH servers
ERROR: pendragon.cs.purdue.edu. claims to be AUTH no NS record
===   from AUTH servers
Checking 2 potential addresses for hosts at mystery.dom.
  == 128.196.128.233 192.12.69.1
in-addr PTR record found for 128.196.128.233
in-addr PTR record found for 192.12.69.1
Summary:
   ERRORS found for mystery.dom. (count: 4)
   WARNINGS issued for mystery.dom. (count: 1)
Done test of mystery.dom.  Fri Apr 27 14:59:07 PDT 1990


4.2.2 Test for mystery domain #2:

Doc-1.0: Starting test of mystery.dom.   parent is edu.
Doc-1.0: Test date - Fri Apr 27 16:14:02 PDT 1990
soa @a.isi.edu. for edu. has serial: 900423
soa @aos.brl.mil. for edu. has serial: 900426
soa @c.nyser.net. for edu. has serial: 900423
soa @gunter-adam.af.mil. for edu. has serial: 900423
soa @ns.nasa.gov. for edu. has serial: 900423
soa @ns.nic.ddn.mil. for edu. has serial: 900426
soa @terp.umd.edu. for edu. has serial: 900426
WARNING: Found 2 unique SOA serial #'s for edu.
Found 3 NS and 3 glue records for mystery.dom. @a.isi.edu.
Found 3 NS and 3 glue records for mystery.dom. @aos.brl.mil.
Found 3 NS and 3 glue records for mystery.dom. @c.nyser.net.
Found 3 NS and 3 glue records for mystery.dom. @gunter-adam.af.mil.
Found 3 NS and 3 glue records for mystery.dom. @ns.nasa.gov.
Found 3 NS and 3 glue records for mystery.dom. @ns.nic.ddn.mil.
Found 3 NS and 3 glue records for mystery.dom. @terp.umd.edu.
DNServers for edu.
   === 0 were also authoritatve for mystery.dom.
   === 7 were non-authoritative for mystery.dom.
Servers for edu. (not also authoritative for mystery.dom.)
   === agree on NS records for mystery.dom.
NS list summary for mystery.dom. from parent (edu.) servers
  == eos.cair.mystery.dom. nike.cair.mystery.dom. ns.utah.edu.
DIGERR (TIME_OUT): dig @eos.cair.mystery.dom. for SOA of mystery.dom.
soa @nike.cair.mystery.dom. for mystery.dom. serial: 60001
soa @ns.utah.edu. for mystery.dom. serial: 60001
SOA serial #'s agree for mystery.dom.
Authoritative domain (mystery.dom.) servers agree on NS for mystery.dom.
ERROR: NS list from mystery.dom. servers do not match parent
  === (edu.) NS list
NS list summary for mystery.dom. from authoritative servers
  == nike.cair.mystery.dom. orion.cair.mystery.dom.
ERROR: ns.utah.edu. claims to be AUTH no NS record from AUTH servers
Checking 1 potential addresses for hosts at mystery.dom.
  == 130.253.1.5
ERROR: no in-addr PTR recorder found for 130.253.1.5
Summary:
   ERRORS found for mystery.dom. (count: 3)
   WARNINGS issued for mystery.dom. (count: 1)
   Incomplete test for mystery.dom. (1)
Done test of mystery.dom.  Fri Apr 27 16:16:08 PDT 1990


References/Readings:

[RFC-1034]      P. Mockapetris, "Domain Names - Concepts and
                Facilities", RFC-1034, USC/Information Sciences
                Institute, November 1987.

[RFC-1035]      P. Mockapetris, "Domain Names - Implementation and
                Specification", RFC-1035, USC/Information Sciences
                Institute, November 1987.


Authors' Addresses:

Steve Hotz
USC - Information Sciences Institute
4676 Admiralty Way
Marina del Rey, Ca. 90293

Phone: (213) 822-1511

Email: hotz@isi.edu


Paul Mockapetris
USC - Information Sciences Institute
4676 Admiralty Way
Marina del Rey, Ca. 90293

Phone: (213) 822-1511

Email: pvm@isi.edu
